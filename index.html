<link rel="stylesheet" href="/path/to/styles/default.min.css">
<script src="/path/to/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<h2><h1 style="color:Tomato;"><center>TorchFire</center></h1></h2>
<p align="center">
<img src="assets\logo.jpg" width="300" height="300" />
</p>
<p>TorchFire strategically synergizes decades of developing machine learning libraries with scientific computation platforms to construct a SciMLCI that is equally appealing to experts in numerical methods, computer science, statistics, and domain scientists/engineers for their SciML research</p>
<h2><strong>Team Members</strong></h2>
<ul>
<li><a href="https://users.oden.utexas.edu/~tanbui/">Tan Bui-Thanh (lead PI, UT Austin)</a><a href="mailto:tanbui@oden.utexas.edu">(Email)</a></li>
<li><a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/dawson">Clint Dawson (UT Austin)</a></li>
<li><a href="https://wccms.oden.utexas.edu/people.html">Michael Sacks (UT Austin)</a></li>
<li><a href="https://www.utfireresearch.com/">Ofodike Ezekoye (UT Austin)</a></li>
<li><a href="https://sites.baylor.edu/robert_kirby/">Robert Kirby (Baylor University)</a></li>
<li><a href="https://www.imperial.ac.uk/people/david.ham">David Ham (Imperial College London)</a></li>
<li><a href="https://www.sandia.gov/ccr/staff/john-n-shadid/">John Shadid (University of New Mexico)</a></li>
<li><a href="https://www.math.uwaterloo.ca/~llramire/CVNov09.pdf">Leticia Ramirez-Ramirez (Center for mathematical investigation, Mexico)</a></li>
</ul>
<h3><strong>Postdoctoral Researchers</strong></h3>
<ul>
<li>TBA</li>
</ul>
<h3><strong>Graduate Researchers</strong></h3>
<ul>
<li>TBA</li>
</ul>
<h3><strong>Undergraduate Researchers</strong></h3>
<ul>
<li>TBA</li>
</ul>
<h3><strong>Summer school opportunities</strong></h3>
<ul>
<li>TBA</li>
</ul>
<h3><strong>Applications</strong></h3>
<p>Forecasting, monitoring, mitigating and controlling of fire spread and cardiovascular problems (CBET, DMS), hurricanes and storm surges (OCE, DMS), infectious disease outbreaks (BIO, CBET, DMS), and magnetohydrodynamics (PHY, DMS)</p>
<h2><strong>Demos and preliminary results</strong></h2>
<h3>Problem settings</h3>
<h4><em>Solving heat equation with neural network:</em></h4>
<p>In this problem, we learn the neural network to solve for the temperature <img src="https://i.upmath.me/svg/u" alt="u" /> given arbitrary conductivity field <img src="https://i.upmath.me/svg/%5Ckappa" alt="\kappa" />. 2D hear equation is written as follows</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%20%20%20-%5Cnabla%20%5Ccdot%20%5Cleft%20.(%20%7Be%5E%5Ckappa%20%5Cnabla%20u%7D%20%5Cright.)%20%26%20%3D%20f%20%20%5Cquad%20%5Ctext%7Bin%20%7D%20%5COmega%20%3D%20%5Cleft%20.%5B%20%7B0%2C1%7D%5E2%20%5Cright.%5D%5C%5C%0A%20%20%20%20%20%20%20%20u%20%26%20%3D%200%20%5Cquad%20%5Ctext%7B%20on%20%7D%20%5CGamma%5E%7B%5Ctext%7Bext%7D%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Ctextbf%7Bn%7D%20%5Ccdot%20%5Cleft.(%7Be%5E%5Ckappa%20%5Cnabla%20u%7D%5Cright.)%20%26%20%3D%200%20%5Cquad%20%5Ctext%7B%20on%20%7D%20%5CGamma%5E%7B%5Ctext%7Broot%7D%7D%2C%0A%5Cend%7Baligned%7D%0A" alt="
\begin{aligned}
         -\nabla \cdot \left .( {e^\kappa \nabla u} \right.) &amp; = f  \quad \text{in } \Omega = \left .[ {0,1}^2 \right.]\\
        u &amp; = 0 \quad \text{ on } \Gamma^{\text{ext}} \\
        \textbf{n} \cdot \left.({e^\kappa \nabla u}\right.) &amp; = 0 \quad \text{ on } \Gamma^{\text{root}},
\end{aligned}
" /></p>
<h4><em>Results: comparison of solutions by Firedrake and TorchFire neural network</em></h4>
<p align="center">
<img src="assets/figures/Heat_eq/Mixed.png" width="500" height="300" >
<p>
<h4><center>Figure 1: Training loss and test accuracy versus the training epochs</center></h4>

<p align="center">
<img src="assets/figures/Heat_eq/Animations.gif" width="1050" height="350">
<p>
<h4><center>Figure 2: (Left) conductivity field, (Middle) True solution obtained by Firedrake software, (Right) Predicted solution obtained by TorchFire neural network</center></h4>

<h4><em>Demo code, TorchFire</em></h4>
<pre><code class="language-python"># Step 0: Import necessary packges
from torchfire import fd_to_torch
import firedrake
import torch
from torch import nn
import pandas

# Step 1: Load train and test data
kappa_train, kappa_test, u_test = pandas.read_csv(...).to_numpy()

# Step 2: TorchFire physic module
## Firedrake Mesh and Space definition
nx, ny = 15, 15
mesh = UnitSquareMesh(nx, ny)
V = FunctionSpace(mesh, &quot;P&quot;, 1)

## Boudary connditions
bc = DirichletBC(V, 0, (1, 2, 3))
## Weak form definition
def assemble_firedrake(u, kappa):
    x = SpatialCoordinate(mesh)
    v = TestFunction(u.function_space())
    f = Constant(20.0)
    return assemble(inner(exp(kappa) * grad(u), grad(v)) * dx - inner(f, v) * dx, bcs=bc)
    
## Torchfire wrapper for assemble_firedrake
templates = (firedrake.Function(V), firedrake.Function(V))
residualTorch = fd_to_torch(assemble_firedrake, templates, &quot;residualTorch&quot;).apply

# Step 3: Neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, kappa_dim, u_dim, neurons):
        super(NeuralNetwork, self).__init__()
        self.Neuralmap1 = nn.Linear(kappa_dim, neurons)
        self.Relu = nn.ReLU()
        self.Neuralmap2 = nn.Linear(neurons, u_dim)
        self.MSE = nn.MSELoss()

    def forward(self, kappa):
        u = self.Neuralmap2(self.Relu(self.Neuralmap1(kappa)))
        return u
        
    def ResidualTorch(self, u, kappa):
        residuals = residualTorch(u, kappa)
        Physic_loss = self.MSE(residuals, torch.zeros_like(residuals))
        return Physic_loss
        
model = NeuralNetwork(kappa_dim = (nx+1)*(ny+1), u_dim = (nx+1)*(ny+1), neurons = 1000)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Step 4 Define train and test function for learning
def train_loop(model, optimizer, kappa):
    train_loss = 0
    for batch in range(int(num_train / batch_size)):
        u_pred = model(kappa[(batch) * batch_size:(batch + 1) * batch_size, :])
        residuals = model.ResidualTorch(u_pred, kappa) / batch_size
        
        # Backpropagation
        optimizer.zero_grad(), residuals.backward(), optimizer.step(),
        train_loss += residuals
    return train_loss
        
def test_loop(model, kappa_test, u_test):
    with torch.no_grad():
        u_pred = model(kappa_test)
        test_acc = (torch.linalg.vector_norm(u_pred - u_test, dim=-1)**2
                    / torch.linalg.vector_norm(u_test, dim=-1)**2).mean()
    return test_acc

for _ in range(nEpochs):
    train_loss = train_loop(model, optimizer, kappa_train)
    test_acc = test_loop(model, kappa_test, u_test)
</code></pre>
